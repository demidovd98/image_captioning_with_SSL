# Image captioning with self-supervised learning for visual feature extraction
A problem of generating a textual description for a given image using self-supervised learned (SSL) approaches as a pre-text task is considered in this work.

1. The code for the project and clear instructions on running the code, 
2. A readme file (text or markdown file) to specify the function of each component (file/folder) in the codebase, 
3. List all dependencies used in the project, and provide instructions on installing any dependencies (e.g., pytorch, cudatoolkit, scikit-learn, etc.) 
4. Provide a demo file available with sample inputs and outputs.
5. Provide instructions on downloading data from publicly available links (for the datasets used in the project)
6. If a project is built on an existing code-base, it must be clearly credited and differences should be explicitly stated in the readme file. 


# Sample Output of Models:
![Captions_For_Updated_Model](https://user-images.githubusercontent.com/45034431/117573575-aeb68d80-b0e9-11eb-9991-4414f0ba6307.JPG)
Captions generated by our explored models with supervised and self-supervised pre-text tasks, trained in similar conditions. We marked the generated captions quality as follows: green colour represents good quality, yellow colour - adequate quality, red - unacceptable quality.

# Problem Statement
Originally, the existed solutions utilised fully supervised trained models for the part of image feature extraction. However, our experiments showed that such a complex task as image captioning requires higher level of generalisation than usual models can provide. This problem could be addressed with using self-supervised learning methods, that recently showed their ability to generalise better.
In order to explore this property of SSL approaches, we proposed and explored two solutions for the image captioning using two different self-supervised learned models, based on Jigsaw Puzzle solving and SimCLR, as a pre-text task.

# Results
For the sake of supervised and self-supervised pre-text tasks comparison, we provide the results of their comprehensive testing on the same downstream task, calculating a BLEU score and validation loss. Our proposed solution with SimCLR model used for image feature extraction achieved the following results: BLEU-1: 0.575, BLEU-2: 0.360, BLEU-3: 0.266, BLEU-4: 0.145, and validation loss of 3.415. These outcomes can be considered as competitive ones with the fully supervised solutions.
Along with the code, we also provide pre-trained models for image captionig task, which can be used for any random image.

# Getting started
Create an conda environment using the dependencies file:

```sh
conda env create -f dependencies.yml
```
# The Contrastive Learning Framework
This folder contains the code and instructions to train jigsaw puzzle pretext task to learn to  extract features, and using it on image captioning downstream task.

## SimCLR pretext task
1. Create the HDF5 dataset (the model expects the data to be in HDF5 file) using the to_hdf5.py specifing the path and desired image size.
2. Create the desired number permutations using maximal_hamming.py in hamming_set by providing number of permutations, which will create a text file with the permutations.
3. In the main.py add the dataset path and the hamming_set (number of permutations) text file path and specify the number of permutations (max_hamming_set) to use from the permutations list (choose the same number if you want to use all the created permutations).
4. In case of using different image size, need to be specified in the dataset creation, datagenerator creation(image_size parameter) and in image_transform ( where crop size is the size of the random crop, cell size  is the size of large patch, and tile size is the final size of the patch).

## Downstream task
1. Using Jigsaw_feature_extraction.py, load the trained model and spacify dataset directory, and choose one of two functions  to extract the features: 
  - Full network uses the last dense layer before the soft max of the whole architecture for features extraction, must specify the same imgae size parameters used in training.
  -  Single network intialize a ResNet50 with the trained weights and use the GAP layer for feature extraction (not recommended).
2. Using jigsaw_vocabulary to generate the vocabulary which generates the descriptions.txt.
2. Using Jigsaw__IC_model to train the captioning model on the extracted features by spicifying the extracted features file location.
3. Jigsaw_blue.py to check the model blue score, requires only the extracted features file location.
4. Jigsaw_test.py used to test the captioning models on images, need specify image location and the image caption model.

# pre-trained models
1. Pre-trained model for jigsaw with Resnet 50 with 67% accuracy on pretext task [Jigsaw_ResNet50](https://mbzuaiac-my.sharepoint.com/:u:/g/personal/20020053_mbzuai_ac_ae/Ed2xPGXaqqpNuQfawHm5HvYBUbW4fL3HNLnTr9HAcrtDvQ?e=3OnR8N)
2. Pre-trained model of image captioning using the jigsaw extracted features [Image_captioning_jigsaw](https://mbzuaiac-my.sharepoint.com/:u:/g/personal/20020053_mbzuai_ac_ae/EXHOb314z-1JlFZKr-umQ8kBOl_A9Q6s3ijJxWxknnheNQ?e=2Q00iC)



# Jigsaw Puzzle solving:
Contains the files neccessary to train jigsaw puzzle solver network following the paper [Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles](https://arxiv.org/abs/1603.09246) and use it for image captioning downstream task.
The impelementation of the code builds on Jeremalloch work in https://github.com/Jeremalloch/Semisupervised_Image_Classifier.
the differences are as follows:
1. Using Alex-net as impelemented in the paper and ResNet50 for shared networks instead of the used Resnet34 and trivial net
2. Our impelentation includes using gray images in training
3. Includes color jittering
4. Applaying normalization on patch level

## Dataset
The dataset used in training is MSCOCO unlabeled 2017,from https://cocodataset.org, can be downoaded here  [MSCOCO unlabeled 2017](http://images.cocodataset.org/zips/unlabeled2017.zip) 
## code components:
#### image_preprocessing:
contains image_transform.py that is used for image preprocessing for jigsaw, which include  the functions to create the croppings and to apply the color jittering
### hamming_set:
Contain maximal_hamming.py which is used to generate the permutations
### to_hdf5.py:
Resizes the dataset and converts it to hdf5 file to be used for training
### Datagnerator.py:
Generate the puzzle patches using image_processing.py to train the keras model 
### Transfer learning for image captioning:
#### contains the files to use jigsaw on the downstream task:
1. Jigsaw_feature_extraction.py extract image features for training using the model provided
2. Jigsaw_vocabulary generate vocabulary of the captions
3. Jigsaw__IC_model.py uses extracted features to train the captioning model
4. Jigsaw_test_blue.py test the model using BLEU score metric
5. jigsaw_test_images.py used to generate descriptions of provided images
This folder contains the code and instructions to train jigsaw puzzle pretext task to learn to  extract features, and using it on image captioning downstream task.

# Instructions on running the code
## Getting started 
Create a conda environment using the dependencies file:

```sh
conda env create -f environment.yml
```
## pretext task
1. Create the HDF5 dataset (the model expects the data to be in HDF5 file) using the to_hdf5.py specifing the path and desired image size.
2. Create the desired number permutations using maximal_hamming.py in hamming_set by providing number of permutations, which will create a text file with the permutations.
3. In the main.py add the dataset path and the hamming_set (number of permutations) text file path and specify the number of permutations (max_hamming_set) to use from the permutations list (choose the same number if you want to use all the created permutations).
4. In case of using different image size, need to be specified in the dataset creation, datagenerator creation(image_size parameter) and in image_transform ( where crop size is the size of the random crop, cell size  is the size of large patch, and tile size is the final size of the patch).

## Downstream task
1. Using Jigsaw_feature_extraction.py, load the trained model and spacify dataset directory, and choose one of two functions  to extract the features: 
  - Full network uses the last dense layer before the soft max of the whole architecture for features extraction, must specify the same imgae size parameters used in training.
  -  Single network intialize a ResNet50 with the trained weights and use the GAP layer for feature extraction (not recommended).
2. Using jigsaw_vocabulary to generate the vocabulary file (descriptions.txt) of the flickr dataset.
2. Using Jigsaw__IC_model to train the captioning model on the extracted features by spicifying the extracted features file location and the descriptions file.
3. Jigsaw_test_blue.py to check the model blue score, requires only the extracted features file location.
4. jigsaw_test_images.py used to test the trained captioning model on images, need specify image location and the used image captioning model.

# pre-trained models
1. Pre-trained model for jigsaw with Resnet 50 with 67% accuracy on pretext task [Jigsaw_ResNet50](https://mbzuaiac-my.sharepoint.com/:u:/g/personal/20020053_mbzuai_ac_ae/Ed2xPGXaqqpNuQfawHm5HvYBUbW4fL3HNLnTr9HAcrtDvQ?e=3OnR8N)
2. Pre-trained model of image captioning using the jigsaw extracted features [Image_captioning_jigsaw](https://mbzuaiac-my.sharepoint.com/:u:/g/personal/20020053_mbzuai_ac_ae/EXHOb314z-1JlFZKr-umQ8kBOl_A9Q6s3ijJxWxknnheNQ?e=2Q00iC)
